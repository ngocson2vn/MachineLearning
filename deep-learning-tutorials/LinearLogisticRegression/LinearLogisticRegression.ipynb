{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Logistic Regression \n",
    "\n",
    "This Jupyter Notebook accompanies the blog posts on Linear and Logistic Regression found [here](https://mukul-rathi.github.io/blog.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we will begin by importing the libraries we will need. \n",
    "__Numpy__ is a linear algebra library in Python and we will be using it to do all of the matrix and vector computations in our code.\n",
    "__Pandas__ is used to import our data and clean it up before we pass it to our machine learning algorithms.\n",
    "__Matplotlib__ will allow us to visualise the training process as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Input\n",
    "\n",
    "Now to import the data - for linear regression. the dataset is on Housing Prices in Boston, sourced from [Kaggle](https://www.kaggle.com/c/boston-housing). This explains which features the abbreviated columns look at. \n",
    "\n",
    "For the purposes of the tutorial, we will split the train.csv file provided by Kaggle into our own train and test set, since the test.csv file doesn't have any labels.\n",
    "\n",
    "**EXTENSION**: run the algorithms on the test.csv (provided in this repo) and submit your predictions to Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
       "0   1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
       "1   2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
       "2   4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
       "3   5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
       "4   7  0.08829  12.5   7.87     0  0.524  6.012  66.6  5.5605    5  311   \n",
       "\n",
       "   ptratio   black  lstat  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     18.7  394.63   2.94  33.4  \n",
       "3     18.7  396.90   5.33  36.2  \n",
       "4     15.2  395.60  12.43  22.9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_dataset = pd.read_csv(\"boston-housing-dataset/train.csv\")\n",
    "house_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to clean up into the format we want:\n",
    "\n",
    "$X$ = _n x m_ matrix, $Y$ = _1 x m_ matrix.\n",
    "\n",
    "We can remove the ID column, since it is not a feature. \n",
    "\n",
    "We will shuffle the data - this is good practice as it ensures the learning is not affected by a particular ordering of data. \n",
    "\n",
    "We normalise it (so the features are in range [0,1] - we do this using formula $$x_{norm} = \\frac{x - \\mu}{\\sigma}$$ where $\\mu$ = mean and $\\sigma$ = standard deviation.\n",
    "\n",
    "We then split the dataset into train:test in an 80:20 split, and then separate the input features from the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_dataset.drop(\"ID\", axis=1, inplace=True)\n",
    "house_dataset = house_dataset.reindex(np.random.permutation(house_dataset.index))\n",
    "\n",
    "#normalise the input features - this ensures they are all in the same range 0-1. \n",
    "house_dataset.loc[:, house_dataset.columns!=\"medv\"] -= house_dataset.loc[:, house_dataset.columns!=\"medv\"].mean()\n",
    "house_dataset.loc[:, house_dataset.columns!=\"medv\"] /= house_dataset.loc[:, house_dataset.columns!=\"medv\"].std()\n",
    "\n",
    "#slice to get train:test split then transpose to get correct dimensions\n",
    "X_lin_train = house_dataset.loc[:house_dataset.shape[0]*4//5, house_dataset.columns!=\"medv\"].values.T \n",
    "Y_lin_train = house_dataset.loc[:house_dataset.shape[0]*4//5, [\"medv\"]].values.T\n",
    "\n",
    "X_lin_test = house_dataset.loc[house_dataset.shape[0]*4//5:, house_dataset.columns!=\"medv\"].values.T\n",
    "Y_lin_test = house_dataset.loc[house_dataset.shape[0]*4//5:, [\"medv\"]].values.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to load the breast cancer dataset - again this is from <a href=\"https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\">Kaggle</a> (though originally from UCI). To contrast with the other dataset, this dataset has features which seem hard to interpret on their own, unlike with housing prices where we can intuitively make sense of the features - e.g more rooms implies higher price. \n",
    "<br> This is where the power of machine learning comes in - to spot patterns in the data not possible by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0     ...               17.33           184.60      2019.0            0.1622   \n",
       "1     ...               23.41           158.80      1956.0            0.1238   \n",
       "2     ...               25.53           152.50      1709.0            0.1444   \n",
       "3     ...               26.50            98.87       567.7            0.2098   \n",
       "4     ...               16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_dataset = pd.read_csv(\"breast-cancer-dataset/data.csv\")\n",
    "cancer_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to clean up the data - remove the id, and convert the label from M/B to 1/0, as well as normalise the data and shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset.drop([\"id\",'Unnamed: 32'], axis=1, inplace=True)\n",
    "cancer_dataset[\"diagnosis\"] = cancer_dataset[\"diagnosis\"].apply(lambda x: 1 if (x==\"M\") else 0)\n",
    "\n",
    "#shuffle data\n",
    "cancer_dataset = cancer_dataset.reindex(np.random.permutation(cancer_dataset.index))\n",
    "\n",
    "#normalise the input features - this ensures they are all in the same range 0-1. \n",
    "cancer_dataset.loc[:, cancer_dataset.columns!=\"diagnosis\"] -= cancer_dataset.loc[:, cancer_dataset.columns!=\"diagnosis\"].mean()\n",
    "cancer_dataset.loc[:, cancer_dataset.columns!=\"diagnosis\"] /= cancer_dataset.loc[:, cancer_dataset.columns!=\"diagnosis\"].std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our $X$ and $Y$ for train and test for both linear and logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log_train = cancer_dataset.loc[:cancer_dataset.shape[0]*4//5, cancer_dataset.columns!=\"diagnosis\"].values.T \n",
    "Y_log_train = cancer_dataset.loc[:cancer_dataset.shape[0]*4//5, [\"diagnosis\"]].values.T \n",
    "\n",
    "X_log_test = cancer_dataset.loc[cancer_dataset.shape[0]*4//5:, cancer_dataset.columns!=\"diagnosis\"].values.T \n",
    "Y_log_test = cancer_dataset.loc[cancer_dataset.shape[0]*4//5:, [\"diagnosis\"]].values.T  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_lin = np.random.randn(Y_lin_train.shape[0], X_lin_train.shape[0])\n",
    "b_lin = np.random.randn(Y_lin_train.shape[0],1)\n",
    "\n",
    "W_log = np.random.randn(Y_log_train.shape[0], X_log_train.shape[0])\n",
    "b_log = np.random.randn(Y_log_train.shape[0],1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we code up the equations for the forward step:\n",
    "    $$ \\hat{Y}_{lin} = WX+b$$\n",
    "    $$ \\hat{Y}_{log} = \\sigma(WX+b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_lin(X, W, b):\n",
    "    return np.dot(W,X)+b\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1+np.exp(-z))\n",
    "\n",
    "def forward_log(X,W,b):\n",
    "    return sigmoid(forward_lin(X,W,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some sample predictions from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.5</td>\n",
       "      <td>-1.586914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.6</td>\n",
       "      <td>5.494474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.9</td>\n",
       "      <td>-2.393939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Y  predictions\n",
       "0  19.5    -1.586914\n",
       "1  22.6     5.494474\n",
       "2  23.9    -2.393939"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"predictions\":forward_lin(X_lin_train[:,:3],W_lin,b_lin)[0] , \"Y\": Y_lin_train[:,:3][0] }).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.209461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.840123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.534362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.060692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y  predictions\n",
       "0  0     0.209461\n",
       "1  0     0.840123\n",
       "2  1     0.828904\n",
       "3  0     0.534362\n",
       "4  1     0.060692"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"predictions\":forward_log(X_log_train[:,:5],W_log,b_log)[0] , \"Y\": Y_log_train[:,:5][0] }).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very good! So let's train the model:\n",
    "\n",
    "First - the **loss functions** need to be defined: these are\n",
    "\n",
    "$$ J(W,b) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "\n",
    "$$ J(W,b) = \\frac{-1}{m} \\sum_{i=1}^{m} y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss(Y, Y_pred):\n",
    "    m = Y.shape[1]\n",
    "    return (1.0/(2*m))*np.sum(np.square(Y_pred-Y))\n",
    "                                      \n",
    "\n",
    "def log_loss(Y, Y_pred):\n",
    "    m = Y.shape[1]\n",
    "    return (-1.0/m)*np.sum(Y*np.log(Y_pred) + (1-Y)*np.log(1-Y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142.44446138745633"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_loss(Y_lin_train,forward_lin(X_lin_train,W_lin,b_lin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to compute the gradients, note that they are actually the same for both linear and logistic regression:\n",
    "$$\\frac{\\partial{J}}{\\partial{W}} =  \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}(\\hat{y}^{(i)} - y^{(i)})$$\n",
    "$$\\frac{\\partial{J}}{\\partial{b}} =  \\frac{1}{m} \\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)}) $$\n",
    "\n",
    "The first equation in matrix form is:\n",
    "\n",
    "$$\\frac{\\partial{J}}{\\partial{W}} =  \\frac{1}{m} (\\hat{Y} - Y).X^{T}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads(X, Y, Y_pred):\n",
    "    m = Y.shape[1]\n",
    "    dW = (1.0/m)*np.dot(Y_pred-Y,X.T)\n",
    "    db = (1.0/m)*np.sum((Y_pred-Y),axis=1,keepdims=True)\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model \n",
    "\n",
    "Finally, we can train the model for both linear and logistic regression.\n",
    "\n",
    "The gradient descent update equation is as follows:\n",
    "\n",
    "$$ W= W - \\alpha \\frac{\\partial \\mathcal{J} }{\\partial W} $$\n",
    "\n",
    "$$ b = b - \\alpha \\frac{\\partial \\mathcal{J} }{\\partial b} $$\n",
    "\n",
    "We will loop over the training set for a set number of iterations (feel free to tweak $\\alpha$ and the number of iterations - these are hyperparameters).\n",
    "We'll also periodically output the loss at that iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_lin(X, W, b, Y, alpha=1e-2, iterations=500):\n",
    "    MSE_losses = [] #keep track of losses to plot\n",
    "    for i in range(iterations):\n",
    "        Y_pred = forward_lin(X, W, b)\n",
    "        MSE_losses.append(MSE_loss(Y,Y_pred)) \n",
    "        if(i%50==0):\n",
    "            print(\"Iteration {}: Loss={}\".format(i, MSE_losses[i]))\n",
    "        dW, db = grads(X, Y, Y_pred)\n",
    "        W = W -  alpha*dW\n",
    "        b = b - alpha*db\n",
    "    #plot the learning curve\n",
    "    plt.plot(range(iterations),MSE_losses)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    return W, b\n",
    "\n",
    "\n",
    "def gradient_descent_log(X, W, b, Y, alpha=3e-2, iterations=1000):\n",
    "    log_losses = [] #keep track of losses to plot\n",
    "    for i in range(iterations):\n",
    "        Y_pred = forward_log(X, W, b)\n",
    "        log_losses.append(log_loss(Y, Y_pred))\n",
    "        if(i%50==0):\n",
    "            print(\"Iteration {}: Loss={}\".format(i, log_losses[i]))\n",
    "        dW, db = grads(X, Y, Y_pred)\n",
    "        W = W -  alpha*dW\n",
    "        b = b - alpha*db\n",
    "    #plot the learning curve\n",
    "    plt.plot(range(iterations),log_losses)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run gradient descent on our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss=262.0815117020298\n",
      "Iteration 50: Loss=92.37719314829103\n",
      "Iteration 100: Loss=42.77267590051945\n",
      "Iteration 150: Loss=22.27903968209717\n",
      "Iteration 200: Loss=13.292711548638371\n",
      "Iteration 250: Loss=9.130594246847615\n",
      "Iteration 300: Loss=7.052356946860518\n",
      "Iteration 350: Loss=5.912583078605908\n",
      "Iteration 400: Loss=5.22192590557804\n",
      "Iteration 450: Loss=4.764288252119233\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d193cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W_lin, b_lin = gradient_descent_lin(X_lin_train, W_lin, b_lin, Y_lin_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss=2.4688115658422487\n",
      "Iteration 50: Loss=0.27787476325631844\n",
      "Iteration 100: Loss=0.1551004857694995\n",
      "Iteration 150: Loss=0.11381878173544119\n",
      "Iteration 200: Loss=0.09184940357069409\n",
      "Iteration 250: Loss=0.07788822478223308\n",
      "Iteration 300: Loss=0.06810060350800586\n",
      "Iteration 350: Loss=0.06079348172040011\n",
      "Iteration 400: Loss=0.05509414560237362\n",
      "Iteration 450: Loss=0.0505028590616163\n",
      "Iteration 500: Loss=0.046710972311958705\n",
      "Iteration 550: Loss=0.04351670724171835\n",
      "Iteration 600: Loss=0.040782171422264035\n",
      "Iteration 650: Loss=0.03840970535996516\n",
      "Iteration 700: Loss=0.03632807388257483\n",
      "Iteration 750: Loss=0.03448400967079236\n",
      "Iteration 800: Loss=0.03283682573364153\n",
      "Iteration 850: Loss=0.03135486685756147\n",
      "Iteration 900: Loss=0.030013104673279712\n",
      "Iteration 950: Loss=0.028791466883314944\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d193128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W_log, b_log = gradient_descent_log(X_log_train, W_log,b_log, Y_log_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test our trained algorithms on the test set:\n",
    "For the logistic regression algorithm, we can measure its performance with accuracy, for linear regression we use MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.35323181847871"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_loss(Y_lin_test, forward_lin(X_lin_test,W_lin,b_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y,Y_pred):\n",
    "    return np.mean(np.abs(np.rint(Y_pred)-Y)) #rint rounds to nearest int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05917159763313609"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(Y_log_test, forward_log(X_log_test,W_log,b_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the accuracy is pretty poor! \n",
    "\n",
    "However, accuracy is not a good metric when the dataset is skewed, instead we look at another metric called the **F1 Score** (ranges between 0 and 1) - higher is better. Using this, it turns out our logistic regression model's performance is pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_Score(Y_pred, Y):\n",
    "    prediction = np.rint(Y_pred)\n",
    "    \n",
    "    truth_pos = (Y == np.ones_like(Y))\n",
    "    truth_neg = (Y == np.zeros_like(Y))\n",
    "    pred_pos = (prediction == np.ones_like(prediction))\n",
    "    pred_neg = (prediction == np.zeros_like(prediction))\n",
    "\n",
    "    true_pos = np.sum(np.logical_and(truth_pos,pred_pos))\n",
    "    if true_pos == 0: #This prevents an undefined computation since precision=recall=0 \n",
    "        return 0\n",
    "    false_pos =np.sum(np.logical_and(truth_neg,pred_pos))\n",
    "    false_neg =np.sum(np.logical_and(truth_pos,pred_neg))\n",
    "    true_neg =np.sum(np.logical_and(truth_neg,pred_neg))\n",
    "\n",
    "    precision = true_pos/(true_pos + false_pos)\n",
    "    recall = (true_pos)/(true_pos + false_neg)\n",
    "    F1_score = 2*(recall*precision) /(recall + precision)\n",
    "    return F1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918918918918919"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_Score(forward_log(X_log_test,W_log,b_log),Y_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
